Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Zhang2016,
abstract = {{\textcopyright} 2016, UK Simulation Society. All rights reserved. This paper presents a system for face reconstruction and warping based on learning from point cloud data captured by Kinect sensor. First, the user is recorded in a natural environment using a low cost 3D acquisition devices--Kinect sensor. Then in order to fix the noisy and incomplete depth data captured by the Kinect sensor, we propose a method to patch the holes in depth image by the symmetric point from the face. In addition, to get enough points to create a 3D face model, a fast point clouds registration method based on SIFT is proposed. Example testifies the effect of the proposed approach.},
author = {Zhang, Jinxiang and Li, Na and Zhang, Jian and Wang, Wanliang and Duan, Ye},
doi = {10.5013/IJSSST.a.17.31.27},
file = {:C$\backslash$:/Users/mbruc/OneDrive/Documents/3D-Scanning-Motion-Capture/Face-Reconstruction/Literature/paper27.pdf:pdf},
issn = {1473804X},
journal = {International Journal of Simulation: Systems, Science and Technology},
keywords = {Face reconstruction,Kinect sensor,Point cloud,SIFT},
number = {31},
pages = {27.1--27.5},
title = {{A study on face reconstruction using data from a low cost sensor}},
volume = {17},
year = {2016}
}
@article{Zollhofer2014,
abstract = {We present a novel method for the interactive markerless reconstruction of human heads using a single commodity RGB-D sensor. Our entire reconstruction pipeline is implemented on the graphics processing unit and allows to obtain high-quality reconstructions of the human head using an interactive and intuitive reconstruction paradigm. The core of our method is a fast graphics processing unit-based nonlinear quasi-Newton solver that allows us to leverage all information of the RGB-D stream and fit a statistical head model to the observations at interactive frame rates. By jointly solving for shape, albedo and illumination parameters, we are able to reconstruct high-quality models including illumination corrected textures. All obtained reconstructions have a common topology and can be directly used as assets for games, films and various virtual reality applications. We show motion retargeting, retexturing and relighting examples. The accuracy of the presented algorithm is evaluated by a comparison against ground truth data. Copyright {\textcopyright} 2014 John Wiley {\&} Sons, Ltd.},
author = {Zollh{\"{o}}fer, Michael and Thies, Justus and Colaianni, Matteo and Stamminger, Marc and Greiner, G{\"{u}}nther},
doi = {10.1002/cav.1584},
file = {:C$\backslash$:/Users/mbruc/OneDrive/Documents/3D-Scanning-Motion-Capture/Face-Reconstruction/Literature/paper.pdf:pdf},
issn = {1546427X},
journal = {Computer Animation and Virtual Worlds},
keywords = {3D scanning,GPGPU,Model-based face reconstruction,Nonlinear optimisation,Statistical head models,Virtual avatars},
number = {3-4},
pages = {213--222},
title = {{Interactive model-based reconstruction of the human head using an RGB-D sensor}},
volume = {25},
year = {2014}
}
@article{Turban2015,
abstract = {The recent success of the Kinect sensor has a significant impact on 3D
data based computer applications. This study aims to obtain MPEG-4
compliant realistic and animatable face models from Kinect video. The
complete framework for this process includes initially the computation
of high quality 3D scans from RGB-D Kinect video, and then the
computation of animatable MPEG-4 face models using these high quality
scans. This study shows that it is possible to obtain high quality 3D
scans and realistic and animatable face models of subjects using lower
quality Kinect data.},
author = {Turban, Laura and Girard, Denis and Kose, Neslihan and Dugelay, Jean Luc},
doi = {10.1109/ICMEW.2015.7169783},
file = {:C$\backslash$:/Users/mbruc/OneDrive/Documents/3D-Scanning-Motion-Capture/Face-Reconstruction/Literature/mm-publi-4592.pdf:pdf},
isbn = {9781479970797},
journal = {2015 IEEE International Conference on Multimedia and Expo Workshops, ICMEW 2015},
keywords = {Animatable face model,Kinect,MPEG-4},
title = {{From Kinect video to realistic and animatable MPEG-4 face model: A complete framework}},
year = {2015}
}
@article{Shin2017,
author = {Shin, Dong-Won and Ho, Yo-Sung},
doi = {10.2352/issn.2470-1173.2016.21.3dipm-408},
file = {:C$\backslash$:/Users/mbruc/OneDrive/Documents/3D-Scanning-Motion-Capture/Face-Reconstruction/Literature/1203.pdf:pdf},
isbn = {9786163618238},
issn = {2470-1173},
journal = {Electronic Imaging},
number = {21},
pages = {1--7},
title = {{Implementation of 3D Object Reconstruction Using Multiple Kinect Cameras}},
volume = {2016},
year = {2017}
}
@article{Bondi2015,
abstract = {Performing face recognition across 3D scans of different resolution is now attracting an increasing interest thanks to the introduction of a new generation of depth cameras, capable of acquiring color/depth images over time. However, these devices have still a much lower resolution than the 3D high-resolution scanners typically used for face recognition applications. If data are acquired without user cooperation, the problem is even more challenging and the gap of resolution between probe and gallery scans can yield to a severe loss in terms of recognition accuracy. Based on these premises, we propose a method to build a higher-resolution 3D face model from 3D data acquired by a low-resolution scanner. This face model is built using data acquired when a person passes in front of the scanner, following an uncooperative protocol. To perform non-rigid registration of point sets and account for deformation of the face during the acquisition process, the Coherent Point Drift (CPD) method is used. Registered 3D data are filtered through a variant of the lowess method to remove outliers and build the final face model. The proposed approach is evaluated in terms of accuracy of face reconstruction and face recognition.},
author = {Bondi, Enrico and Pala, Pietro and Berretti, Stefano and {Del Bimbo}, Alberto},
doi = {10.1109/fg.2015.7284882},
file = {:C$\backslash$:/Users/mbruc/OneDrive/Documents/3D-Scanning-Motion-Capture/Face-Reconstruction/Literature/tifs16{\_}bondi.pdf:pdf},
number = {12},
pages = {1--6},
title = {{Reconstructing high-resolution face models from Kinect depth sequences acquired in uncooperative contexts}},
volume = {11},
year = {2015}
}
